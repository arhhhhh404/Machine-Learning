APPRENTISSAGE SUPERVISÉ:

L’apprentissage supervisé est un type d’apprentissage automatique dans lequel on fournit au modèle des données d’entrée (X) accompagnées de leurs étiquettes de sortie (y). L’objectif est que le modèle apprenne à prédire la sortie y correspondante à une nouvelle entrée x, après avoir observé suffisamment d’exemples (x, y).

Les éléments de base de l’apprentissage supervisé sont:
-les données (X, y): une collection d’exemples (x1, y1), (x2, y2), ..., (xn, yn) où chaque xᵢ est un vecteur d’entrée et yᵢ est l’étiquette associée
-un modèle: une fonction paramétrée (f) qui approxime la relation entre les entrées x et les sorties y
-un critère de perte (loss): une fonction (L) qui mesure l’erreur entre la prédiction du modèle et la vraie sortie, que l’on cherche à minimiser
-un algorithme d’optimisation: méthode qui ajuste les paramètres du modèle pour minimiser la perte sur les données d’entraînement

Il existe 2 types principaux que l'on peut effectuer à l'aide de l'apprentissage supervisé:
-Classification:
    Prédire une catégorie parmi un ensemble fini
-Régression:
    Prédire une valeur numérique continue

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
base:
    -model = regression(...) (créer un objet avec un estimateur et ses hyperparamètre)
    -model.fit(X, y) (entraine le modèle sur les données X et y divisé donc en 2 tableau numpy avec X et y de dim=2)
    -model.score(X,y) (évalue le modèle)
    -model.predict(X) (utilise le modèle)

train et test:
    train test split:
        -import model_selection.train_test_split (on importe de quoi faire un train test)
        -X_train, X_test, y_train, y_test = train_test_split(X, y) (on divise la part de données d'entrainement et de test)
    cross validation:
        -import cross_val_score (on importe de quoi faire un validation test)
        -cross_val_score(instruction(), X_train, y_train, cv=cv, scoring="métrique") (estime les validation test selon une metrique un certain nombre de fois n)
            kfold:
            -import kFold (on importe de quoi utiliser la méthode kfold)
            -cv = Kfold(n, random_state=0) (on découpe n split de manière parfaitement égal et aléatoire (pour dataset d'entrainement équilibrer))
            shuffle split:
            -import ShuffleSplit (on importe de quoi utiliser la méthode de shuffle split)
            -cv = SuffleSplit(n, test_size=) (on définie une portion de donnée pour l'entraienement et la validation avant de regrouper les données et recommencer le tout de manière aléatoire n fois (pour dataset d'entrainement déséquilibrer))
            stratified kfold:
            -import StratifiedkFold (on importe de quoi utiliser la méthode de stratified kfold)
            -cv = StratifiedkFold(n) (creer n split en regroupant une certaine proportion des différentes classe (pour dataset d'entrainement dont il manque une classe))
            group kfold:
            -import GroupkFold (on importe de quoi utiliser la méthode de group kfold)
            -cv = GroupkFold(n).get_n_split(X, y, groups=) (on créer n split en mélangeant des groupes déterminer par la relation que ses éléments ont entre eux (pour les dataset d'entrainement ou ))
    validation curve:
        -import validation_curve (on importe de quoi faire une validation curve)
        -train_score, val_score = validation_curve(model, X_train, y_train, cv=n) (estime le score du set d'entrainent et du set validation)
    grid search:
        -import GridSearshCV (on importe de quoi faire trouver les meilleur paramètre)
        -grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5) (test les différent paramètre pour estimer le meilleur)
    learning_curve:
        -import learning_curve (on importe de quoi tracer une courbe d'apprentissage)
        -N, train_score, val_score = learning_curve(model, X_train, y_train, train_size=n, cv=5) (évalue le nombre de point selon le nombre de donnée du train set et du validation set)

métrique et régression:
    -from sklearn.metrics import * (on importe de quoi faire une moyenne des erreur, absolue, carré, racine carré)
        -mean_squared_error(y, y_pred) (on opère la MSE soit la moyenne des erreurs soit y - y_pred au carré (on l'uilise quand les erreur sont grande))
        -mean_absolute_error(y, y_pred) (on opère la MAE soit la moyenne absolue des erreurs soit y - y_pred (on l'uilise quand les erreur sont linéaires))
        -median_absolute_error(y, y_pred) ((on opère la médiane absolue des erreurs soit y - y_pred (très peu sensible aux grandes erreurs))
    -from sklearn.metrics import make_scorer (on importe de quoi utiliser make_scorer)
        -make_scorer(function, greater_is_better=True) (on créer notre propre métrique en fonction des erreur dans la fonction avec le paramètre qui dit que plus on a de bon pourcentage mieux c'est)

pre-processing:
    transformer et estimer:
        -transformer.fit(X_train) (on trouve une fonction de transformation à partir des données de X_train (transformation))
        -transformer.transform(X_train) (on applique la fonction de transformation sur les données de X_train (estimation))
        -transformer.fit_transform(X_train) (on fait les deux étapes vu ci dessus)
    Encodage:
    -from sklearn.preprocessing import LAbelEncoder, OrdinalEncoder, LabelBinarizer, MultiLabelBinarizer, OneHotEncoder (on importe de quoi encoder)
        -encoder = LabelEncoder() ou OrdinalEncoder() ou LabelBinarizer() ou MultiLabelBinarizer() ou OneHotEncoder() (va créer un modèle ou on associe chaque catégorie ou classe d'une variable à une valeur décimal unique (Label quand on à une seul variable et ordinal quand on en a plusieurs, de plus si on ne veut pas de relation d'ordre on utilise l'encodage one hot))
        -encoder.fit_transform(y)
        -encoder.inverse_transform(y) (on fait un désencodage)
    normaliser:
    -from sklearn.preprocessing import MinMaxscaler, StandardScaler, RobustScaler (on importe de quoi normaliser)
        -scaler = MinMaxScaler() ou StandardScaler() ou RobustScaler (va créer un modèle ou la valeur des données vont changer de sorte à pouvoir etre mieux réutiliser (pour des valeurs abérrante on utilise RobustScaler sinon le reste marche))
        -scaler.fit_transforme(X)
    discrétisation:
    -from sklearn.preprocessing import Binarizer, KbinsDiscretizer
        -disc = Binarizer(threshold=n) ou Kbinsdiscretizer(n_bins=n) (divise une variable en deux catégorie (pour Binarizer) ou plusieurs (pour KbinsDiscretizer) selon un seuil en hyperparamètre)
        -disc.fit_transform(X) 
    polynomial:
    -from sklearn.preprocessing import PolynomialFeatures
        -polynome = PolynomialFeatures(d) (va créer un polynome de degré d avec pour coef les variable d'un dataset avec une colonne 1, x et x²)
        -polynome.fit_transform(X)
    pipeline:
    -from sklearn.pipeline import make_pipeline, make_column_transform, make_union (on importe de quoi opérer une pipeline)
        -model = make_pipeline(StandardScaler(), SGDCClassifier()) ou make_column_transformer((StandardScaler(), ["column1", "column2" etc..])) (on automatise une chaine d'étape de structuration pour un modèle sur des colonne(make_column_transform) ou sur les data entièrement (make_pipeline))
        -numerical_pipeline = make_pipeline(SimpleImputer(), StandardScaler()) et categorical pipeline = make_pipeline(SimpleImputer(strategy = OneHotEncoder())) (on créer deux pipeline a injecter dans le make_columna_transformer)
        -make_union(StandardScaler(), Binarizer()) (on peut faire deux transformation)
    imputer:
    -from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator (on importe de quoi imputer avec Simple, KNN et MissingIndicator)
        -imputer = SimpleImputer(missing_values=np.nan, strategy="mean") ou KNNImputer(n_neighbors=1) (on remplace les valeurs manquante par une valeur statistique ou regarde la cohérence avec les voisins pour estimer la valeurs manquante)
        -MissingIndicator() (va retrouver les valeurs manquantes en revoiyant une valeur booléenne)
    feature selection:
    -from sklearn.feature_selection import VarianceThreshold (on importe de quoi utiliser VarianceThreshold)
        selector = VarianceThreshold(threshold=) (on élimine les variable dont la variance est inférieur au seuil de threshold)
    -from sklearn.feature_selection import SelectKBest (on importe de quoi utiliser SelectKbest)
        selector = SelectBBest(chi2, k=) (on sélectionne les K (k=noombre) variable X dont le score du test de dépendance chi2 avec le y le plys élevé)
    -from sklearn.feature_selection import SelectFromModel (on importe de quoi utiliser SelectFrom Model)
        selector = SelectFromModel(estimateur, type) (on entraine un estimateur puis on sélectionne les variables les plus importante pour cet estimateur (compatible avec les estimateur qui sont paramétrée))
    -from sklearn.feature_selection import RFE, RFECV
        selector = RFE(estimateur, step=, min_feature_to_select=) (on élimine les variable les moins importante de facon récursive avec un estimateur, le nombre de variable quon souhaite éléminer et le nombre de variable qu'on souhaite avoir à la fin (RFECV est quand on rajoute les couche de valiation))
